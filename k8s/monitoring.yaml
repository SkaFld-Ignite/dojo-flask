# ServiceMonitor for Prometheus (if using Prometheus Operator)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ai-video-chaptering-metrics
  namespace: ai-video-chaptering
  labels:
    app: ai-video-chaptering
spec:
  selector:
    matchLabels:
      app: backend-api
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
---
# PodMonitor for Celery workers
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: celery-worker-metrics
  namespace: ai-video-chaptering
  labels:
    app: ai-video-chaptering
spec:
  selector:
    matchLabels:
      app: celery-worker
  podMetricsEndpoints:
  - port: metrics
    path: /metrics
    interval: 30s
---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-video-chaptering-dashboard
  namespace: ai-video-chaptering
  labels:
    grafana_dashboard: "1"
data:
  dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "AI Video Chaptering",
        "tags": ["ai", "video", "chaptering"],
        "timezone": "browser",
        "panels": [
          {
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(flask_http_request_total[5m])",
                "legendFormat": "{{method}} {{endpoint}}"
              }
            ]
          },
          {
            "title": "Response Time",
            "type": "graph", 
            "targets": [
              {
                "expr": "flask_http_request_duration_seconds",
                "legendFormat": "{{method}} {{endpoint}}"
              }
            ]
          },
          {
            "title": "Celery Queue Length",
            "type": "graph",
            "targets": [
              {
                "expr": "celery_queue_length",
                "legendFormat": "{{queue}}"
              }
            ]
          },
          {
            "title": "Active Workers",
            "type": "stat",
            "targets": [
              {
                "expr": "celery_workers_active",
                "legendFormat": "Active Workers"
              }
            ]
          },
          {
            "title": "Video Processing Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(video_processing_total[5m])",
                "legendFormat": "Videos Processed/sec"
              }
            ]
          },
          {
            "title": "AI Model Performance",
            "type": "graph",
            "targets": [
              {
                "expr": "ai_model_inference_duration_seconds",
                "legendFormat": "{{model}}"
              }
            ]
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "5s"
      }
    }
---
# PrometheusRule for alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ai-video-chaptering-alerts
  namespace: ai-video-chaptering
  labels:
    app: ai-video-chaptering
spec:
  groups:
  - name: ai-video-chaptering
    rules:
    - alert: HighErrorRate
      expr: rate(flask_http_request_total{status=~"5.."}[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value }} errors per second"
    
    - alert: HighResponseTime
      expr: flask_http_request_duration_seconds{quantile="0.95"} > 5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High response time detected"
        description: "95th percentile response time is {{ $value }} seconds"
    
    - alert: CeleryQueueBacklog
      expr: celery_queue_length > 100
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Celery queue backlog"
        description: "Queue length is {{ $value }} tasks"
    
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod is crash looping"
        description: "Pod {{ $labels.pod }} is restarting frequently"
    
    - alert: HighMemoryUsage
      expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage"
        description: "Memory usage is {{ $value | humanizePercentage }} of limit"
    
    - alert: DatabaseConnectionFailure
      expr: up{job="postgres"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Database connection failure"
        description: "Cannot connect to PostgreSQL database"
    
    - alert: RedisConnectionFailure
      expr: up{job="redis"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Redis connection failure"  
        description: "Cannot connect to Redis"
    
    - alert: StorageSpaceLow
      expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Storage space low"
        description: "Available storage is {{ $value | humanizePercentage }} of total" 